{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9250f97",
   "metadata": {},
   "source": [
    "# Exploring Dandiset 000690: Allen Institute Openscope - Vision2Hippocampus project\n",
    "\n",
    "> **DISCLAIMER**: This notebook was AI-generated and has not been fully verified. Please exercise caution when interpreting the code or results, and verify important findings independently.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The [Allen Institute Openscope - Vision2Hippocampus project](https://dandiarchive.org/dandiset/000690) investigates how visual information is processed and transformed as it travels from the thalamus through visual cortical areas to the hippocampus in mice. This project aims to understand how neural representations of simple and natural visual stimuli evolve across this processing pathway.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Explore the structure of the Dandiset\n",
    "2. Examine electrophysiological data from a Neuropixels probe recording\n",
    "3. Analyze neural spiking activity across brain regions\n",
    "4. Investigate neural responses to various visual stimuli\n",
    "5. Visualize the relationship between visual stimuli and neural activity\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "This notebook requires the following Python packages:\n",
    "\n",
    "- pynwb\n",
    "- h5py\n",
    "- remfile\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b32cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import remfile\n",
    "import pynwb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style for most plots\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124a640",
   "metadata": {},
   "source": [
    "## Accessing the Dandiset\n",
    "\n",
    "We'll use the DANDI API to access the Dandiset and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a047093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "# Connect to DANDI archive\n",
    "client = DandiAPIClient()\n",
    "dandiset = client.get_dandiset(\"000690\")\n",
    "\n",
    "# Print basic information about the Dandiset\n",
    "metadata = dandiset.get_raw_metadata()\n",
    "print(f\"Dandiset name: {metadata['name']}\")\n",
    "print(f\"Dandiset URL: {metadata['url']}\")\n",
    "\n",
    "# List the assets in the Dandiset\n",
    "assets = list(dandiset.get_assets())\n",
    "print(f\"\\nFound {len(assets)} assets in the dataset\")\n",
    "print(\"\\nFirst 5 assets:\")\n",
    "for asset in assets[:5]:\n",
    "    print(f\"- {asset.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4c6de",
   "metadata": {},
   "source": [
    "## Dataset Structure\n",
    "\n",
    "This Dandiset contains extracellular electrophysiology recordings using Neuropixels probes in mice. The recordings were performed while presenting different visual stimuli to the mice.\n",
    "\n",
    "The main file types include:\n",
    "\n",
    "1. **Main session files** (e.g., `sub-692072_ses-1298465622.nwb`) - Contains metadata, units (spikes), and other session information\n",
    "2. **Image files** (e.g., `sub-692072_ses-1298465622_image.nwb`) - Contains visual stimulus templates\n",
    "3. **Probe-specific ecephys files** (e.g., `sub-692072_ses-1298465622_probe-0_ecephys.nwb`) - Contains LFP data for specific probes\n",
    "\n",
    "Let's first examine a probe's LFP data to understand brain activity during the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccd4d6",
   "metadata": {},
   "source": [
    "## Exploring LFP Data from a Neuropixels Probe\n",
    "\n",
    "Local Field Potentials (LFPs) represent the summed synaptic activity of neurons near the recording electrode. Let's load LFP data from one of the probes and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for probe 0 data from subject 692072\n",
    "probe_url = \"https://api.dandiarchive.org/api/assets/ba8760f9-91fe-4c1c-97e6-590bed6a783b/download/\"\n",
    "print(f\"Neurosift link: https://neurosift.app/nwb?url={probe_url}&dandisetId=000690&dandisetVersion=draft\")\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data from remote NWB file...\")\n",
    "remote_file = remfile.File(probe_url)\n",
    "h5_file = h5py.File(remote_file)\n",
    "io = pynwb.NWBHDF5IO(file=h5_file)\n",
    "nwb = io.read()\n",
    "\n",
    "print(f\"Session ID: {nwb.session_id}\")\n",
    "print(f\"Session description: {nwb.session_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a97f4",
   "metadata": {},
   "source": [
    "### Electrode Information\n",
    "\n",
    "Let's examine the electrodes used in this recording, including their positions and associated brain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dae0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get electrode information\n",
    "electrodes_df = nwb.electrodes.to_dataframe()\n",
    "print(f\"Number of electrodes: {len(electrodes_df)}\")\n",
    "print(\"\\nSample of electrode data:\")\n",
    "print(electrodes_df.head())\n",
    "\n",
    "# Get information about brain regions represented\n",
    "brain_regions = electrodes_df['location'].unique()\n",
    "print(f\"\\nBrain regions in recording: {brain_regions}\")\n",
    "\n",
    "# Plot brain region counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "electrodes_df['location'].value_counts().plot(kind='bar')\n",
    "plt.title('Number of Electrodes per Brain Region')\n",
    "plt.xlabel('Brain Region')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot electrode positions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    electrodes_df['probe_horizontal_position'], \n",
    "    electrodes_df['probe_vertical_position'],\n",
    "    c=electrodes_df.index, \n",
    "    cmap='viridis', \n",
    "    alpha=0.8,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(label='Electrode index')\n",
    "plt.xlabel('Horizontal position (µm)')\n",
    "plt.ylabel('Vertical position (µm)')\n",
    "plt.title('Probe electrode positions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb156a",
   "metadata": {},
   "source": [
    "### LFP Data\n",
    "\n",
    "Now let's look at the structure of the LFP data and visualize some example traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f20b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to LFP data\n",
    "probe_0_lfp = nwb.acquisition['probe_0_lfp']\n",
    "probe_0_lfp_data = probe_0_lfp.electrical_series['probe_0_lfp_data']\n",
    "\n",
    "print(f\"LFP data shape: {probe_0_lfp_data.data.shape}\")\n",
    "print(f\"LFP time points: {len(probe_0_lfp_data.timestamps)}\")\n",
    "print(f\"LFP sampling rate: {1/(probe_0_lfp_data.timestamps[1] - probe_0_lfp_data.timestamps[0])} Hz\")\n",
    "print(f\"LFP duration: {probe_0_lfp_data.timestamps[-1] - probe_0_lfp_data.timestamps[0]} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00534ec",
   "metadata": {},
   "source": [
    "Let's visualize some LFP traces from different brain regions to see regional differences in activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8002448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample of LFP data from different channels\n",
    "# Sample 1 second of data starting at an interesting point\n",
    "start_time = 10000  # Starting sample\n",
    "sample_length = 1250  # 1 second at 1250 Hz\n",
    "\n",
    "# Sample every Nth channel to get a representative set\n",
    "N = 20  # Take every 20th channel\n",
    "sampled_channels = list(range(0, probe_0_lfp_data.data.shape[1], N))[:5]  # Take up to 5 channels\n",
    "\n",
    "# Get sample data for selected channels\n",
    "sample_data = probe_0_lfp_data.data[start_time:start_time+sample_length, sampled_channels]\n",
    "sample_time = probe_0_lfp_data.timestamps[start_time:start_time+sample_length]\n",
    "\n",
    "# Plot sample LFP traces\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, channel_idx in enumerate(sampled_channels):\n",
    "    # Offset each trace for visibility\n",
    "    offset = i * 0.0005  # Adjust based on data amplitude\n",
    "    plt.plot(\n",
    "        sample_time, \n",
    "        sample_data[:, i] + offset, \n",
    "        label=f'Channel {channel_idx}'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('LFP (V)')\n",
    "plt.title('Sample LFP traces from different channels')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd08b8",
   "metadata": {},
   "source": [
    "Let's also visualize the LFP activity across all channels as a heatmap to get a better sense of the spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of LFP activity across channels\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(\n",
    "    sample_data.T, \n",
    "    aspect='auto',\n",
    "    extent=[sample_time[0], sample_time[-1], 0, len(sampled_channels)-1],\n",
    "    origin='lower', \n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.colorbar(label='LFP (V)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Channel index')\n",
    "plt.title('LFP activity across selected channels')\n",
    "plt.yticks(range(len(sampled_channels)), [f'Ch {ch}' for ch in sampled_channels])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf87049",
   "metadata": {},
   "source": [
    "## Exploring Spiking Activity\n",
    "\n",
    "Now let's examine the spiking activity (units) from the main session file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the main session file\n",
    "session_url = \"https://api.dandiarchive.org/api/assets/fbcd4fe5-7107-41b2-b154-b67f783f23dc/download/\"\n",
    "print(f\"Neurosift link: https://neurosift.app/nwb?url={session_url}&dandisetId=000690&dandisetVersion=draft\")\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading session data from remote NWB file...\")\n",
    "remote_file = remfile.File(session_url)\n",
    "h5_file = h5py.File(remote_file)\n",
    "io = pynwb.NWBHDF5IO(file=h5_file)\n",
    "session_nwb = io.read()\n",
    "\n",
    "print(f\"Session ID: {session_nwb.session_id}\")\n",
    "print(f\"Session description: {session_nwb.session_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b089e2",
   "metadata": {},
   "source": [
    "### Unit Information\n",
    "\n",
    "Let's explore the properties of the recorded units (neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ed64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get units data\n",
    "units_df = session_nwb.units.to_dataframe()\n",
    "print(f\"Number of units: {len(units_df)}\")\n",
    "print(\"\\nSample of units data:\")\n",
    "print(units_df[['firing_rate', 'quality', 'isi_violations']].head())\n",
    "\n",
    "# Plot firing rate distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(units_df['firing_rate'].clip(upper=50), bins=50, kde=True)\n",
    "plt.xlabel('Firing Rate (Hz, clipped at 50 Hz)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Neuron Firing Rates')\n",
    "plt.show()\n",
    "\n",
    "# Plot quality metrics distribution (if available)\n",
    "if 'quality' in units_df.columns and units_df['quality'].dtype != object:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='quality', data=units_df)\n",
    "    plt.title('Distribution of Unit Quality')\n",
    "    plt.xlabel('Quality')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad5b41",
   "metadata": {},
   "source": [
    "### Spike Raster Plot\n",
    "\n",
    "Let's create a raster plot to visualize spike times for a few example units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few units with different firing rates\n",
    "units_by_firing_rate = units_df.sort_values('firing_rate', ascending=False)\n",
    "sample_units = units_by_firing_rate.iloc[[0, 10, 50, 100, 250]].index\n",
    "\n",
    "# Get spike times for sample units\n",
    "sample_spike_times = {}\n",
    "for unit_id in sample_units:\n",
    "    spike_times = units_df.loc[unit_id, 'spike_times']\n",
    "    # Only keep spike times in a reasonable range to visualize\n",
    "    # For example, 10 seconds of data starting at time 100\n",
    "    sample_spike_times[unit_id] = spike_times[(spike_times >= 100) & (spike_times < 110)]\n",
    "\n",
    "# Plot the raster\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (unit_id, spike_times) in enumerate(sample_spike_times.items()):\n",
    "    plt.scatter(\n",
    "        spike_times, \n",
    "        np.ones_like(spike_times) * i, \n",
    "        s=10, \n",
    "        label=f\"Unit {unit_id} (FR: {units_df.loc[unit_id, 'firing_rate']:.2f} Hz)\"\n",
    "    )\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.yticks(range(len(sample_units)), [f\"Unit {id}\" for id in sample_units])\n",
    "plt.ylabel('Unit')\n",
    "plt.title('Spike Raster Plot for Sample Units (10 sec window)')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966a3a9",
   "metadata": {},
   "source": [
    "## Visual Stimuli\n",
    "\n",
    "Now let's examine the visual stimuli used in the experiment. The stimuli are stored in the image NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the image file\n",
    "image_url = \"https://api.dandiarchive.org/api/assets/cbc64387-19b9-494a-a8fa-04d3207f7ffb/download/\"\n",
    "print(f\"Neurosift link: https://neurosift.app/nwb?url={image_url}&dandisetId=000690&dandisetVersion=draft\")\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading image data from remote NWB file...\")\n",
    "remote_file = remfile.File(image_url)\n",
    "h5_file = h5py.File(remote_file)\n",
    "io = pynwb.NWBHDF5IO(file=h5_file)\n",
    "image_nwb = io.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629dcdf",
   "metadata": {},
   "source": [
    "### Stimulus Templates\n",
    "\n",
    "Let's look at the stimulus templates available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a01075",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Look at the stimulus template section\n",
    "stimulus_templates = image_nwb.stimulus_template\n",
    "print(f\"Available stimulus templates: {list(stimulus_templates.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b63ce",
   "metadata": {},
   "source": [
    "The stimuli in this dataset fall into several categories:\n",
    "\n",
    "1. **SAC (and variants)**: Simple moving bars with different parameters\n",
    "2. **Disco2SAC**: Colorful disco-like moving bars\n",
    "3. **Ring**: Ring-shaped stimuli\n",
    "4. **Disk**: Disk-shaped stimuli\n",
    "5. **natmovie**: Natural movies of various scenes\n",
    "\n",
    "The stimuli are parameterized with properties like:\n",
    "- **Wd**: Width (e.g., 15 or 45 degrees)\n",
    "- **Vel**: Velocity (e.g., 2 or 8 degrees/second)\n",
    "- **Bndry**: Boundary conditions for the stimulus\n",
    "- **Cntst**: Contrast level\n",
    "\n",
    "Let's visualize examples from different stimulus categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378da250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display frames from a stimulus\n",
    "def display_stimulus_frames(stim_name, num_frames=4):\n",
    "    if stim_name in stimulus_templates:\n",
    "        stim_template = stimulus_templates[stim_name]\n",
    "        print(f\"\\nStimulus: {stim_name}\")\n",
    "        print(f\"Shape: {stim_template.data.shape}\")\n",
    "        print(f\"Rate: {stim_template.rate} Hz\")\n",
    "        \n",
    "        # Determine if it's a RGB stimulus or grayscale\n",
    "        is_rgb = len(stim_template.data.shape) == 4\n",
    "\n",
    "        # Get frames at regular intervals\n",
    "        if is_rgb:\n",
    "            n_frames = stim_template.data.shape[2]\n",
    "            frame_indices = np.linspace(0, n_frames-1, num_frames, dtype=int)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, num_frames, figsize=(16, 4))\n",
    "            for i, frame_idx in enumerate(frame_indices):\n",
    "                frame = stim_template.data[:, :, frame_idx, :]\n",
    "                axes[i].imshow(frame)\n",
    "                axes[i].set_title(f\"Frame {frame_idx}\")\n",
    "                axes[i].axis('off')\n",
    "        else:\n",
    "            n_frames = stim_template.data.shape[2]  \n",
    "            frame_indices = np.linspace(0, n_frames-1, num_frames, dtype=int)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, num_frames, figsize=(16, 4))\n",
    "            for i, frame_idx in enumerate(frame_indices):\n",
    "                frame = stim_template.data[:, :, frame_idx]\n",
    "                axes[i].imshow(frame, cmap='gray')\n",
    "                axes[i].set_title(f\"Frame {frame_idx}\")\n",
    "                axes[i].axis('off')\n",
    "                \n",
    "        plt.suptitle(f\"{stim_name.replace('_presentations', '')}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Stimulus template not found for {stim_name}\")\n",
    "\n",
    "# Display example stimuli\n",
    "example_stimuli = [\n",
    "    \"SAC_Wd15_Vel2_Bndry1_Cntst0_loop_presentations\",  # Standard bar stimulus\n",
    "    \"Disco2SAC_Wd15_Vel2_Bndry1_Cntst0_loop_presentations\",  # Disco bar stimulus\n",
    "    \"SAC_Wd45_Vel2_Bndry1_Cntst0_loop_presentations\",  # Wide bar stimulus\n",
    "    \"natmovie_EagleSwooping1_540x960Full_584x460Active_presentations\"  # Natural movie\n",
    "]\n",
    "\n",
    "for stim_name in example_stimuli:\n",
    "    display_stimulus_frames(stim_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e755152",
   "metadata": {},
   "source": [
    "### Stimulus Presentations\n",
    "\n",
    "Now let's examine when these stimuli were presented during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e17a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at presentation intervals\n",
    "interval_names = list(session_nwb.intervals.keys())\n",
    "print(f\"Number of stimulus presentation intervals: {len(interval_names)}\")\n",
    "print(f\"Sample of interval names: {interval_names[:5]}\")\n",
    "\n",
    "# Group stimulus names by pattern\n",
    "stimulus_types = {}\n",
    "for name in interval_names:\n",
    "    if \"_presentations\" not in name:\n",
    "        continue\n",
    "    \n",
    "    base_name = name.replace(\"_presentations\", \"\")\n",
    "    parts = base_name.split(\"_\")\n",
    "    \n",
    "    # Group by first part of name (stimulus type)\n",
    "    stim_type = parts[0]\n",
    "    if stim_type not in stimulus_types:\n",
    "        stimulus_types[stim_type] = []\n",
    "    \n",
    "    stimulus_types[stim_type].append(name)\n",
    "\n",
    "# Print stimulus categories\n",
    "print(\"\\nStimulus categories:\")\n",
    "for stim_type, intervals in stimulus_types.items():\n",
    "    print(f\"- {stim_type}: {len(intervals)} variants\")\n",
    "\n",
    "# Plot number of variants per stimulus type\n",
    "plt.figure(figsize=(12, 6))\n",
    "counts = [len(intervals) for stim_type, intervals in stimulus_types.items()]\n",
    "plt.bar(stimulus_types.keys(), counts)\n",
    "plt.xlabel('Stimulus Type')\n",
    "plt.ylabel('Number of Variants')\n",
    "plt.title('Number of Stimulus Variants per Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecdcc8",
   "metadata": {},
   "source": [
    "Let's examine the timing of stimulus presentations for one example stimulus type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cef5a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get presentation intervals for a sample stimulus\n",
    "sample_stim_name = next(iter(stimulus_types['SAC']))\n",
    "presentation_df = session_nwb.intervals[sample_stim_name].to_dataframe()\n",
    "\n",
    "print(f\"\\nSample of stimulus presentation intervals for {sample_stim_name}:\")\n",
    "print(presentation_df.head())\n",
    "\n",
    "# Plot the presentation times\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(presentation_df['start_time'], np.zeros_like(presentation_df['start_time']), \n",
    "            alpha=0.5, marker='|', s=80)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.yticks([])\n",
    "plt.title(f'Presentation Times for {sample_stim_name.replace(\"_presentations\", \"\")}')\n",
    "plt.xlim(presentation_df['start_time'].min(), \n",
    "         min(presentation_df['start_time'].min() + 300, presentation_df['start_time'].max()))\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of stimulus durations\n",
    "plt.figure(figsize=(10, 6))\n",
    "durations = presentation_df['stop_time'] - presentation_df['start_time']\n",
    "sns.histplot(durations, bins=50, kde=True)\n",
    "plt.xlabel('Duration (s)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Distribution of Stimulus Durations for {sample_stim_name.replace(\"_presentations\", \"\")}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7517519",
   "metadata": {},
   "source": [
    "## Analyzing Neural Responses to Visual Stimuli\n",
    "\n",
    "Now let's analyze how neurons respond to the visual stimuli. We'll look at a simple example of spike-triggered averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902e096",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to compute peri-stimulus time histogram (PSTH)\n",
    "def compute_psth(spike_times, event_times, window=(-0.5, 1.5), bin_size=0.01):\n",
    "    \"\"\"\n",
    "    Compute peri-stimulus time histogram for a given unit around stimulus events.\n",
    "    \n",
    "    Args:\n",
    "        spike_times: spike times for a single unit\n",
    "        event_times: stimulus presentation times\n",
    "        window: time window around each event (in seconds)\n",
    "        bin_size: bin size for histogram (in seconds)\n",
    "    \n",
    "    Returns:\n",
    "        bins: bin centers for the histogram\n",
    "        psth: peri-stimulus time histogram\n",
    "    \"\"\"\n",
    "    # Create bins\n",
    "    bins = np.arange(window[0], window[1] + bin_size, bin_size)\n",
    "    bin_centers = bins[:-1] + bin_size/2\n",
    "    \n",
    "    # Initialize PSTH\n",
    "    psth = np.zeros(len(bin_centers))\n",
    "    \n",
    "    # For each event, find spikes within the window\n",
    "    for event_time in event_times:\n",
    "        # Find spikes relative to this event\n",
    "        relative_spikes = spike_times - event_time\n",
    "        # Filter to spikes within the window\n",
    "        mask = (relative_spikes >= window[0]) & (relative_spikes < window[1])\n",
    "        filtered_spikes = relative_spikes[mask]\n",
    "        \n",
    "        # Bin the spikes\n",
    "        hist, _ = np.histogram(filtered_spikes, bins=bins)\n",
    "        \n",
    "        # Add to PSTH\n",
    "        psth += hist\n",
    "    \n",
    "    # Normalize by number of events and bin size to get firing rate\n",
    "    psth = psth / (len(event_times) * bin_size)\n",
    "    \n",
    "    return bin_centers, psth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28456b84",
   "metadata": {},
   "source": [
    "Let's select a stimulus and compute PSTHs for a few units to see how they respond to the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific stimulus type\n",
    "stim_name = sample_stim_name  # Use the same stimulus from above\n",
    "\n",
    "# Get stimulus presentation times\n",
    "stim_times = session_nwb.intervals[stim_name].to_dataframe()['start_time'].values\n",
    "\n",
    "# Select a few units to analyze\n",
    "high_fr_units = units_by_firing_rate.head(5).index\n",
    "\n",
    "# Compute and plot PSTHs\n",
    "fig, axes = plt.subplots(len(high_fr_units), 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "for i, unit_id in enumerate(high_fr_units):\n",
    "    spike_times = units_df.loc[unit_id, 'spike_times']\n",
    "    \n",
    "    # Compute PSTH\n",
    "    bin_centers, psth = compute_psth(spike_times, stim_times, window=(-0.2, 1.0), bin_size=0.01)\n",
    "    \n",
    "    # Plot PSTH\n",
    "    axes[i].bar(bin_centers, psth, width=0.01, alpha=0.7)\n",
    "    axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.6)  # Mark stimulus onset\n",
    "    axes[i].set_ylabel('Firing Rate (Hz)')\n",
    "    axes[i].set_title(f'Unit {unit_id} (Firing Rate: {units_df.loc[unit_id, \"firing_rate\"]:.2f} Hz)')\n",
    "    \n",
    "    # Set ylim to better visualize responses\n",
    "    if psth.max() > 0:\n",
    "        axes[i].set_ylim(0, psth.max() * 1.2)\n",
    "\n",
    "# Add labels\n",
    "axes[-1].set_xlabel('Time from Stimulus Onset (s)')\n",
    "plt.suptitle(f'Neural Responses to {stim_name.replace(\"_presentations\", \"\")}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d054e",
   "metadata": {},
   "source": [
    "## Analyzing Responses to Different Stimulus Types\n",
    "\n",
    "Let's compare how a single unit responds to different types of stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a unit that had a strong response\n",
    "unit_id = high_fr_units[0]  # Choose the unit with the highest firing rate\n",
    "\n",
    "# Choose different stimulus types\n",
    "stim_types = ['SAC', 'Disco2SAC', 'natmovie']\n",
    "sample_stim_names = []\n",
    "\n",
    "for stim_type in stim_types:\n",
    "    if stim_type in stimulus_types and len(stimulus_types[stim_type]) > 0:\n",
    "        sample_stim_names.append(stimulus_types[stim_type][0])\n",
    "\n",
    "# Compute and plot PSTHs for each stimulus type\n",
    "fig, axes = plt.subplots(len(sample_stim_names), 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "for i, stim_name in enumerate(sample_stim_names):\n",
    "    # Get stimulus presentation times\n",
    "    stim_times = session_nwb.intervals[stim_name].to_dataframe()['start_time'].values\n",
    "    \n",
    "    # Get spike times for the selected unit\n",
    "    spike_times = units_df.loc[unit_id, 'spike_times']\n",
    "    \n",
    "    # Compute PSTH\n",
    "    bin_centers, psth = compute_psth(spike_times, stim_times, window=(-0.2, 1.0), bin_size=0.01)\n",
    "    \n",
    "    # Plot PSTH\n",
    "    axes[i].bar(bin_centers, psth, width=0.01, alpha=0.7)\n",
    "    axes[i].axvline(x=0, color='r', linestyle='--', alpha=0.6)  # Mark stimulus onset\n",
    "    axes[i].set_ylabel('Firing Rate (Hz)')\n",
    "    axes[i].set_title(f'Response to {stim_name.replace(\"_presentations\", \"\")}')\n",
    "    \n",
    "    # Set ylim to better visualize responses\n",
    "    if psth.max() > 0:\n",
    "        axes[i].set_ylim(0, psth.max() * 1.2)\n",
    "\n",
    "# Add labels\n",
    "axes[-1].set_xlabel('Time from Stimulus Onset (s)')\n",
    "plt.suptitle(f'Unit {unit_id} Responses to Different Stimuli', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa589b7",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "In this notebook, we explored Dandiset 000690 from the Allen Institute Openscope - Vision2Hippocampus project. This dataset contains recordings from multiple Neuropixels probes in mice, capturing neural activity across various brain regions while presenting different visual stimuli.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. The dataset includes recordings from multiple brain regions, including visual cortex, thalamus, and hippocampus.\n",
    "2. The LFP signals show distinct patterns across different brain regions, reflecting regional differences in neural activity.\n",
    "3. Units (neurons) exhibit a wide range of firing rates, with most neurons firing at low rates (< 5 Hz).\n",
    "4. The dataset includes an extensive set of visual stimuli, including simple moving bars with various parameters, disco-colored bars, and natural movies.\n",
    "5. Neural responses to visual stimuli show interesting temporal patterns, with some units exhibiting clear stimulus-locked responses.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "This notebook provides a starting point for more in-depth analyses of this rich dataset. Possible future directions include:\n",
    "\n",
    "1. Comparing neural responses across different brain regions to understand how visual information is transformed along the processing pathway.\n",
    "2. Analyzing how specific stimulus parameters (width, velocity, contrast) affect neural responses.\n",
    "3. Comparing responses to artificial stimuli versus natural movies to understand how the brain processes natural scenes differently.\n",
    "4. Examining synchronization between brain regions during stimulus presentation.\n",
    "5. Applying advanced analyses like dimensionality reduction or encoding models to understand population-level representations of visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a397a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
