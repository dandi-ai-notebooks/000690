{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c106d5c",
   "metadata": {},
   "source": [
    "# Exploring Dandiset 000690: Vision2Hippocampus Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5248024",
   "metadata": {},
   "source": [
    "**Note:** This notebook was AI-generated and has not been fully verified. Please be cautious when interpreting the code or results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cf2c3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook explores data from the Allen Institute's OpenScope - Vision2Hippocampus project (Dandiset 000690). The project investigates how neural representations of visual stimuli evolve from the thalamus through visual cortex to hippocampus in mice.\n",
    "\n",
    "Key details:\n",
    "- **Dandiset URL:** [https://dandiarchive.org/dandiset/000690/0.250326.0015](https://dandiarchive.org/dandiset/000690/0.250326.0015)\n",
    "- **Subjects:** 3 mice (subject 692072 in this notebook)\n",
    "- **Stimuli:** Simple visual motion (bars of light) and complex naturalistic stimuli (movies)\n",
    "- **Techniques:** Multi-electrode extracellular electrophysiology recordings (Neuropixels 1.0 probes)\n",
    "- **Data types:** LFP, spike sorted units, stimulus information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03894fba",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "To run this notebook, you'll need:\n",
    "- dandi\n",
    "- pynwb\n",
    "- h5py\n",
    "- remfile \n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014df928",
   "metadata": {},
   "source": [
    "## Loading the Dandiset\n",
    "First we'll connect to the DANDI archive and load metadata about this Dandiset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DANDI archive\n",
    "client = DandiAPIClient()\n",
    "dandiset = client.get_dandiset(\"000690\", \"0.250326.0015\")\n",
    "\n",
    "# Print basic information about the Dandiset\n",
    "metadata = dandiset.get_raw_metadata()\n",
    "print(f\"Dandiset name: {metadata['name']}\")\n",
    "print(f\"Dandiset description: {metadata['description']}\")\n",
    "print(f\"Subjects: {[s['subject_id'] for s in metadata['variableMeasured'] if 'subject_id' in s]}\")\n",
    "\n",
    "# List some assets in the Dandiset\n",
    "assets = dandiset.get_assets()\n",
    "print(\"\\nFirst 5 assets:\")\n",
    "for asset in islice(assets, 5):\n",
    "    print(f\"- {asset.path} (ID: {asset.identifier})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858898fa",
   "metadata": {},
   "source": [
    "## Loading NWB File\n",
    "We'll examine data from the first probe (probe 0) of subject 692072. This contains LFP recordings from a Neuropixels probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8670288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynwb\n",
    "import h5py\n",
    "import remfile\n",
    "\n",
    "# Load the NWB file\n",
    "url = \"https://api.dandiarchive.org/api/assets/ba8760f9-91fe-4c1c-97e6-590bed6a783b/download/\"\n",
    "remote_file = remfile.File(url)\n",
    "h5_file = h5py.File(remote_file)\n",
    "io = pynwb.NWBHDF5IO(file=h5_file)\n",
    "nwb = io.read()\n",
    "\n",
    "# Print basic file info\n",
    "print(f\"Session ID: {nwb.session_id}\")\n",
    "print(f\"Subject ID: {nwb.subject.subject_id}\")\n",
    "print(f\"Age: {nwb.subject.age}\")\n",
    "print(f\"Probe: {nwb.devices['probeA'].description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665a724",
   "metadata": {},
   "source": [
    "## NWB File Structure\n",
    "This NWB file contains LFP data recorded from a Neuropixels probe. Key components:\n",
    "\n",
    "- **acquisition/probe_0_lfp_data**: LFP data (10117092 timepoints Ã— 95 channels)\n",
    "- **electrodes**: Table with metadata about each recording channel\n",
    "- **devices/probeA**: Information about the Neuropixels 1.0 probe\n",
    "- **subject**: Information about the mouse subject\n",
    "\n",
    "Explore this NWB file in Neurosift: [https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/ba8760f9-91fe-4c1c-97e6-590bed6a783b/download/&dandisetId=000690&dandisetVersion=draft](https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/ba8760f9-91fe-4c1c-97e6-590bed6a783b/download/&dandisetId=000690&dandisetVersion=draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c846c13",
   "metadata": {},
   "source": [
    "### Electrode Information\n",
    "Let's examine the electrode metadata to understand the recording setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get electrodes table as pandas DataFrame\n",
    "electrodes_df = nwb.electrodes.to_dataframe()\n",
    "\n",
    "# Show basic electrode statistics\n",
    "print(f\"Number of electrodes: {len(electrodes_df)}\")\n",
    "print(\"Electrode locations:\")\n",
    "print(electrodes_df['location'].value_counts())\n",
    "\n",
    "# Visualize electrode positions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(electrodes_df['y'], electrodes_df['z'], c=electrodes_df['x'], cmap='viridis')\n",
    "plt.colorbar(label='X coordinate (posterior)')\n",
    "plt.xlabel('Y (inferior)')\n",
    "plt.ylabel('Z (right)')\n",
    "plt.title('Electrode Positions in Brain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e470f",
   "metadata": {},
   "source": [
    "## Visualizing LFP Data\n",
    "We'll now load and visualize a small subset of the LFP data. Since the full dataset is very large (>10 million timepoints), we'll analyze a short segment from the middle of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e94324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LFP data\n",
    "lfp = nwb.acquisition['probe_0_lfp_data']\n",
    "fs = nwb.electrode_groups['probeA'].lfp_sampling_rate  # Sampling rate (Hz)\n",
    "\n",
    "# Plot a subset of the data (60 sec segment from middle of recording)\n",
    "start_idx = len(lfp.timestamps) // 2  # Middle of recording\n",
    "duration = 60  # seconds\n",
    "n_samples = int(duration * fs)\n",
    "\n",
    "# Load data for 10 channels (every 10th channel)\n",
    "channel_idx = [i for i in range(0, 95, 10)]\n",
    "data_samples = lfp.data[start_idx:start_idx+n_samples, channel_idx]\n",
    "times = lfp.timestamps[start_idx:start_idx+n_samples]\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, ch in enumerate(channel_idx):\n",
    "    offset = i * 0.5  # Offset traces for visualization\n",
    "    plt.plot(times, data_samples[:, i] + offset, \n",
    "             label=f'Ch {ch} ({electrodes_df.iloc[ch][\"location\"]})')\n",
    "    \n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Voltage (offset)')\n",
    "plt.title(f'LFP Traces ({duration}s segment)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8d9cd",
   "metadata": {},
   "source": [
    "## Summary and Future Directions\n",
    "This notebook demonstrated how to:\n",
    "- Access data from the Vision2Hippocampus project on DANDI\n",
    "- Load and explore NWB files containing Neuropixels recordings\n",
    "- Visualize electrode positions and LFP data\n",
    "\n",
    "### Potential Next Steps:\n",
    "- Analyze responses to specific visual stimuli (available in other NWB files in this dataset)\n",
    "- Compare activity patterns across different brain regions\n",
    "- Compute frequency-domain features from the LFP (e.g., power spectra)\n",
    "- Combine with spike data (available in other files) for multi-scale analysis"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
